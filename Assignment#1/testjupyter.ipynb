{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c9a2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2\n",
      "  3   4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#R: np.array (num_states, num_actions)\n",
    "#rows correspond to states, columns correspond to actions\n",
    "#R(s1,a1) = 1, R(s1,a2) = 2, R(s2,a1) = 3, R(s2,a2) = 4\n",
    "R = np.array([[1,2],[3,4]])\n",
    "for row in R:\n",
    "    print(\" \".join(f\"{val:3}\" for val in row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0765965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T: np.array (num_states, num_actions, num_states)\n",
    "#T[s,a,s‚Ä≤]=P(s‚Ä≤‚à£s,a)\n",
    "T = np.array([\n",
    "            [[0.5, 0.5], \n",
    "             [0.8, 0.2]],\n",
    "            [[0.2, 0.8], \n",
    "             [0.3, 0.7]]\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "852154c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.array([10,20])\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d90c9c",
   "metadata": {},
   "source": [
    "Computing the Bellman backup equation\n",
    "v(s1) = max(Q(s1,a1), Q(s1,a2))\n",
    "v(s1) = max(Q(s2,a1), Q(s2,a2))\n",
    "\n",
    "Q(s1,a1) = R(s1,a1) + gamma * p(s1,a1,s1) * v(s1)  + gamma * p(s1,a1,s2) * V(s2) \n",
    "\n",
    "![alt text](<bellman equation 1.JPG>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b976060",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "Reward <bold>function is usually r(s) or r(s,a) or even r(s,a,s'). In the above example it is r(s,a). The immediate reward R when taken action a from state s. R = 3 is the immediate reward for taking action a1 from state s2.\n",
    "p = transitional matrix (T (s,a,s'))\n",
    "\n",
    "![alt text](<Q calculation.JPG>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "097d6690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "[[15.85 13.88]\n",
      " [20.82 20.83]]\n",
      "[15.85 20.83]\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((2,2))\n",
    "print(Q)\n",
    "Q = R + gamma * np.sum(T * V, axis=2)\n",
    "print(Q)\n",
    "\n",
    "V = np.max(Q, axis=1)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0572dc",
   "metadata": {},
   "source": [
    "Rule of matrix multiplication\n",
    "\n",
    "If you multiply a matrix of shape:\n",
    "( ùê¥ √ó ùêµ )\n",
    "with a matrix of shape \n",
    "( ùêµ √ó ùê∂)\n",
    "\n",
    "you get a new matrix of shape:\n",
    "( ùê¥ √ó ùê∂ )\n",
    "\n",
    "üëâ The inner dimensions (B) must match. The result has the outer dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df764784",
   "metadata": {},
   "source": [
    "# POLICY\n",
    "\n",
    "policy: np.array (num_states)\n",
    "R: np.array (num_states, num_actions)\n",
    "T: np.array (num_states, num_actions, num_states)\n",
    "gamma: float\n",
    "tol: float\n",
    "\n",
    "![alt text](<policy reward.JPG>) \n",
    "\n",
    "![alt text](<policy transition.JPG>) \n",
    "\n",
    "![alt text](<policy evaluation - 1.JPG>) \n",
    "\n",
    "![alt text](<policy evaluation - 2.JPG>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c74a48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base Test - Setup\n",
    "R = np.array([[1, 2], [3, 4]])\n",
    "T = np.array([\n",
    "            [[0.5, 0.5], [0.8, 0.2]],\n",
    "            [[0.2, 0.8], [0.3, 0.7]]\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d864b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b-0-basic: Policy evaluation expected type and shape\n",
    "#Deterministic policy\n",
    "policy = np.array([0, 1])\n",
    "gamma = 0\n",
    "V = np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "34eb0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. 14.]\n",
      " [12. 24.]]\n",
      "[15. 38.]\n",
      "[17. 36.]\n"
     ]
    }
   ],
   "source": [
    "#Demo np.sum\n",
    "a = np.array([[0.3,0.7],[0.4,0.6]])\n",
    "b = np.array([[10,20],[30,40]])\n",
    "print(a * b)\n",
    "c = np.sum(a * b, axis=0)\n",
    "d = np.sum(a * b, axis=1)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d35a1b",
   "metadata": {},
   "source": [
    "![alt text](lecture_policy_evaluation_Bellman.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "36955155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy: [0 1]\n",
      "T: [[[0.5 0.5]\n",
      "  [0.8 0.2]]\n",
      "\n",
      " [[0.2 0.8]\n",
      "  [0.3 0.7]]]\n",
      "R: [[1 2]\n",
      " [3 4]]\n",
      "Q: [[1. 2.]\n",
      " [3. 4.]]\n",
      "policy one hot: [[1. 0.]\n",
      " [0. 1.]]\n",
      "Q: [[1. 2.]\n",
      " [3. 4.]]\n",
      "V new: [1. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(\"policy:\", policy)\n",
    "print(\"T:\", T)\n",
    "print(\"R:\",R)\n",
    "\n",
    "tol = 1e-3\n",
    "diff = np.inf\n",
    "while(diff > tol):\n",
    "    Q = R + gamma * np.sum(T * V, axis=2)\n",
    "    print(\"Q:\", Q)\n",
    "    if policy.ndim == 1:\n",
    "        policy_one_hot = np.zeros_like(Q)\n",
    "        policy_one_hot[np.arange(Q.shape[0]), policy] = 1\n",
    "        policy = policy_one_hot\n",
    "        print(\"policy one hot:\", policy)\n",
    "    V_new = np.sum(policy * Q, axis=1)\n",
    "    diff = np.max(np.abs(V_new - V))\n",
    "    V = V_new\n",
    "print(\"V new:\", V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
